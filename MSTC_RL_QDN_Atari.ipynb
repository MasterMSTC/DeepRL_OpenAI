{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MSTC_RL_QDN_Atari.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sgXdaJu6RPMW","colab_type":"text"},"cell_type":"markdown","source":["# <font color=#3876f1  >Introducing DEEP Reinforement Learning Playing ATARI GAMES</font>\n","![Atari Breakthrough](https://cdn-images-1.medium.com/max/1600/1*bq0g9O26bO4nLvRwNw9mpw.gif)\n","## <font color=green>[MTSC](http://www.mstc.ssr.upm.es/big-data-track) : Applications Project Course</font>"]},{"metadata":{"id":"EsaSJR29brwC","colab_type":"text"},"cell_type":"markdown","source":["---\n","## <font color=#d35400>Remember from...</font>\n","## [*Human-level control through deep reinforcement learning*, Volodymyr Mnih, et al.](https://www.nature.com/articles/nature14236/)\n","  Nature volume 518, pages 529–533 (26 February 2015)\n","  \n","<img src=https://ocr.space/blog/images/posts/old/ai-nature-cover.jpg height=\"200\" width=\"200\">\n","  \n","**Editorial Summary**: *Self-taught AI agent masters Atari arcade games*\n","\n","\n",">   - For an artificial agent to be considered truly intelligent it needs to excel at a variety of tasks considered challenging for humans. \n","\n",">   - To date, it has only been possible … to master a single discipline for example, IBM's Deep Blue beat the human world champion at chess…\n","\n",">   - Now a team working at Google's DeepMind subsidiary has developed an artificial agent dubbed a deep Q-network that learns to play 49 classic Atari 2600 'arcade' games directly from sensory experience, achieving performance on a par with that of an expert human player. \n","\n","\n","By **combining reinforcement learning** (selecting actions that maximize reward in this case the game score) **with deep learning** (multilayered feature extraction from high-dimensional data in this case the pixels), **the game-playing agent takes artificial intelligence a step nearer the goal of systems capable of learning a diversity of challenging tasks from scratch.**\n","\n"]},{"metadata":{"id":"Q9ME1297yeAO","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## We will use the library: <font color=magenta size=5>[OpenAI Gym](https://gym.openai.com/docs/)</font>\n","\n","- OpenAI is a company created by Elon Musk (*cofoundader of PayPal, Tesla Motors, SpaceX, Hyperloop, SolarCity, The Boring Company, OpenAI*) that has been doing research in deep reinforcement learning.\n","\n","![OpenAI](https://cdn-images-1.medium.com/max/800/1*pPxE7B-vSNOMtv_veQz4Tw.png)"]},{"metadata":{"id":"0xCe7NfiySag","colab_type":"text"},"cell_type":"markdown","source":["---\n","Adapted from several sources:\n",">   **MAINLY Tatsuya** (tokb23) : https://github.com/tokb23/dqn/blob/master/dqn.py\n","\n",">   https://github.com/llSourcell/deep_q_learning/blob/master/03_PlayingAgent.ipynb\n","\n",">   https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n","\n","---\n"]},{"metadata":{"id":"G6B3CBDa_Fbs","colab_type":"text"},"cell_type":"markdown","source":["- # <font color=green>FIRST: let's install everything</font>"]},{"metadata":{"id":"ceED_nzKyLwC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["! apt-get update"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wN7DZfseMLH2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["! apt-get install build-essential -y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WV7K07RR9-nu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["! apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig > /dev/null"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KJwqh37t0wJ0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["! pip install --upgrade pip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"exd0VV451wFS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["! pip install gym[atari]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cbVKTcIvVoSg","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","- # <font color=green>Now let's know about OpenAI Gym</font>"]},{"metadata":{"id":"BEiicCISVoSi","colab_type":"text"},"cell_type":"markdown","source":["### There are two basic concepts in reinforcement learning:\n","- ### the environment (namely, the outside world)\n","- ### and the agent (namely, the algorithm you are writing).\n","\n","      The agent sends actions to the environment, and the environment replies with observations (state) and rewards (that is, a score).\n"]},{"metadata":{"id":"YuCtW_efVoSk","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","##<font color=magenta>State</font>\n","### The simplest approximation of a $state$ is simply the current frame in your Atari game.\n","![Several frames](http://o7ie0tcjk.bkt.clouddn.com/rl/games/croped-breakout-image.png)\n","\n","- ### <font color=red>How many states do we have?... What type of dynamics?</font>\n"]},{"metadata":{"id":"-FxOCTX9VoSm","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","##<font color=magenta>Action</font>\n","**A total of 18 actions can be performed with the joystick**: doing nothing, pressing the action button, going in one of 8 directions (up, down, left and right as well as the 4 diagonals) and going in any of these directions while pressing the button.\n","\n","- ### In Breakout, only 4 actions apply:\n","\n","> 1.- do nothing\n","\n","> 2.- Fire: “asking for a ball” at the beginning of the game by pressing the button\n","\n","> 3.- going left\n","\n","> 4.- going right\n","\n"]},{"metadata":{"id":"SGwvXCflVoSo","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","##<font color=magenta>Reward</font>\n","In Atari, **rewards simply correspond to changes in score**."]},{"metadata":{"id":"WmELyhe_VoSq","colab_type":"text"},"cell_type":"markdown","source":["---\n","##<font color=darkblue>The GYM interface</font>\n","[OpenAI Gym Documentation](https://github.com/openai/gym#id15)\n","      \n","**The core gym interface is Env**, which is the unified environment interface.\n","\n","The following are some **Env methods**:\n","\n","\n","*   **env = gym.make('Breakout-v0')** # creates an instance\n","*   **reset(self)**: Reset the environment's state. Returns observation.\n","*   **step(self, action)**: Step the environment by one timestep. Returns observation, reward, done, info.\n","\n"]},{"metadata":{"id":"SjOLxGx9VoSu","colab_type":"text"},"cell_type":"markdown","source":["---\n","*To be discussed:*\n","\n","*   **render(self, mode='human', close=False)**: Render one frame of the environment. The default mode will do something human friendly, such as pop up a window. Passing the close flag signals the renderer to close any such windows.\n","\n","---\n"]},{"metadata":{"id":"Kfb9fR47WqeM","colab_type":"text"},"cell_type":"markdown","source":["<font color=darkblue size=4>**Let's see some examples:**</font>\n","\n"]},{"metadata":{"id":"RRZJTMJdVoSw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import time\n","import gym\n","from IPython import display\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make('Breakout-v0')\n","\n","env.reset()\n","\n","plt.imshow(env.render(mode='rgb_array'))\n","action=0\n","observation, reward, done, info = env.step(action)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iTrd7lfGVoTC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.imshow(observation)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wT62Z1kpVoTK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print('Reward:' ,reward)\n","print('Observation: ', observation.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GDLywJpDXdrQ","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","##<font color=magenta>A RANDOM PLayer</font>\n","**See playing by a random selection of actions**."]},{"metadata":{"id":"lbq-pxDNVoTS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import time\n","import gym\n","from IPython import display\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make('Breakout-v4')\n","#env = gym.make('Assault-ram-v0')\n","#env = gym.make('MsPacman-ram-v0')\n","\n","env.reset()\n","for _ in range(20):\n","    plt.imshow(env.render(mode='rgb_array'))\n","    display.display(plt.gcf())\n","    time.sleep(1)\n","    display.clear_output(wait=True)\n","    action = env.action_space.sample() # select a random action\n","\n","    observation, reward, done, info = env.step(action)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yfNoM2kiVoTa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["    print(\"Action space (i.e. no. actions): \",env.action_space)\n","    print(\"Observation space: \",env.observation_space)\n","    print(action)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n5J31RU7VoTg","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","<font color=darkorange>All our efforts from now on will be about **replacing the random action selection** in the code above with something more *sensible!*\n","---\n","\n"]},{"metadata":{"id":"LmE0Oi4nYOHW","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","- # <font color=green>We need some THEORETICAL KNOWLEGE!</font>\n","\n","##  Read DeepMind papers:\n","- ## [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) which introduces the notion of a Deep Q-Network.\n","- ## [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)\n","\n","## And follow our [REINFORCEMENT LEARNING Course](http://www.mstc.ssr.upm.es/images/enrollment2017-2018/GA_09AT_93000948_2S_2017-18.pdf)"]},{"metadata":{"id":"UbFnWfu0Z8DS","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","##<font color=magenta>Revisiting : State</font>\n","      We said: The simplest approximation of a $state$ is simply the current frame in your Atari game.\n","\n","### Unfortunately, this is not always sufficient: given the image on the left, you are probably unable to tell whether the ball is going up or going down! <font color=brown> breaking the Markov property!</font>\n","![Several frames](https://cdn-images-1.medium.com/max/600/1*Me-kiRwUc1b5_GZszN2UaQ.gif)\n","\n","- ###  A simple trick to deal with this is simply to bring some of the previous history into your state (that is perfectly acceptable under the Markov property). DeepMind chose to use the past 4 frames <font color=red>*(know why?)* </font>"]},{"metadata":{"id":"8rx86FscYOHg","colab_type":"text"},"cell_type":"markdown","source":["### <font color=1334F2>Preprocessing</font>\n","\n","    Working directly with raw Atari frames, which are 210×160 pixel images with a 128 color palette, can be computationally demanding, so ...\n","\n","- We crope to 84×84** region: the implementation of 2D convolutions that we are going to be using can handle rectangular inputs easily.\n","\n","**See the code below (do not execute it!)**"]},{"metadata":{"id":"Args8Nv8bleo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["ENV_NAME = 'Breakout-v0'  # Environment name\n","FRAME_WIDTH = 84  # Resized frame width\n","FRAME_HEIGHT = 84  # Resized frame height\n","\n","def preprocess(observation, last_observation):\n","    processed_observation = np.maximum(observation, last_observation)\n","    processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n","    return np.reshape(processed_observation, (FRAME_WIDTH, FRAME_HEIGHT,1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o3Yp2njXcYuq","colab_type":"text"},"cell_type":"markdown","source":["## <font color=1334F2>SOME CODING INFORMATION</font>\n","\n","> ### 1.- We are goint to use both: KERAS and TENSORFLOW\n","> ### 2.- To make the code more readable we will use A [Python CLASS](https://docs.python.org/2/tutorial/classes.html) for our agent\n","<br>\n","\n","<font color=darkorange size=4>**We will follow**:<br>\n"," **MAINLY Tatsuya** (tokb23) : https://github.com/tokb23/dqn/blob/master/dqn.py</font>\n","\n","\n","\n","(Python classes provide all the standard features of Object Oriented Programming: the class inheritance mechanism allows multiple base classes, a derived class can override any methods of its base class or classes, and a method can call the method of a base class with the same name.)\n","> ### 3.- ...and we are going to have a LARGE set of configration parameters!\n","\n","\n","\n","---"]},{"metadata":{"id":"0NH5jAhxWDNO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import os\n","import gym\n","import random\n","import numpy as np\n","import tensorflow as tf\n","from collections import deque\n","from skimage.color import rgb2gray\n","from skimage.transform import resize\n","from keras.models import Sequential\n","from keras.layers import Convolution2D, Flatten, Dense"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B7OPNzDxWNLA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["ENV_NAME = 'Breakout-v0'  # Environment name\n","FRAME_WIDTH = 84  # Resized frame width\n","FRAME_HEIGHT = 84  # Resized frame height\n","STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n","\n","#NUM_EPISODES = 12000  # Number of episodes the agent plays\n","NUM_EPISODES = 2000  # Number of episodes the agent plays\n","\n","GAMMA = 0.99  # Discount factor\n","EXPLORATION_STEPS = 1000000  # Number of steps over which the initial value of epsilon is linearly annealed to its final value\n","INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n","FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n","\n","#INITIAL_REPLAY_SIZE = 20000  # Number of steps to populate the replay memory before training starts\n","#NUM_REPLAY_MEMORY = 400000  # Number of replay memory the agent uses for training\n","\n","INITIAL_REPLAY_SIZE = 5000  # Number of steps to populate the replay memory before training starts\n","NUM_REPLAY_MEMORY = 40000  # Number of replay memory the agent uses for training\n","\n","BATCH_SIZE = 32  # Mini batch size\n","#TARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\n","TARGET_UPDATE_INTERVAL = 1000  # The frequency with which the target network is updated\n","\n","TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n","LEARNING_RATE = 0.00025  # Learning rate used by RMSProp\n","MOMENTUM = 0.95  # Momentum used by RMSProp\n","MIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n","\n","SAVE_INTERVAL = 300000  # The frequency with which the network is saved\n","NO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n","LOAD_NETWORK = False\n","TRAIN = True\n","SAVE_NETWORK_PATH = 'saved_networks/' + ENV_NAME\n","SAVE_SUMMARY_PATH = 'summary/' + ENV_NAME\n","\n","NUM_EPISODES_AT_TEST = 30 # Number of episodes the agent plays at test time\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mE6Fzi0CWk6A","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class Agent():\n","    def __init__(self, num_actions):\n","        self.num_actions = num_actions\n","        self.epsilon = INITIAL_EPSILON\n","        self.epsilon_step = (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n","        self.t = 0\n","\n","        # Parameters used for summary\n","        self.total_reward = 0\n","        self.total_q_max = 0\n","        self.total_loss = 0\n","        self.duration = 0\n","        self.episode = 0\n","\n","        # Create replay memory\n","        self.replay_memory = deque()\n","\n","        # Create q network\n","        self.s, self.q_values, q_network = self.build_network()\n","        q_network_weights = q_network.trainable_weights\n","\n","        # Create target network\n","        self.st, self.target_q_values, target_network = self.build_network()\n","        target_network_weights = target_network.trainable_weights\n","\n","        # Define target network update operation\n","        self.update_target_network = \\\n","            [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n","\n","        # Define loss and gradient update operation\n","        self.a, self.y, self.loss, self.grads_update = self.build_training_op(q_network_weights)\n","\n","        self.sess = tf.InteractiveSession()\n","        self.saver = tf.train.Saver(q_network_weights)\n","        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n","        self.summary_writer = tf.summary.FileWriter(SAVE_SUMMARY_PATH, self.sess.graph)\n","\n","        if not os.path.exists(SAVE_NETWORK_PATH):\n","            os.makedirs(SAVE_NETWORK_PATH)\n","\n","        self.sess.run(tf.global_variables_initializer())\n","\n","        # Load network\n","        if LOAD_NETWORK:\n","            self.load_network()\n","\n","        # Initialize target network\n","        self.sess.run(self.update_target_network)\n","\n","    def build_network(self):\n","        model = Sequential()\n","        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), activation='relu', \\\n","                                input_shape=(FRAME_WIDTH, FRAME_HEIGHT, STATE_LENGTH)))\n","        \n","        # FOR THEANO : input_shape=(STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT)))\n","        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), activation='relu'))\n","        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu'))\n","        model.add(Flatten())\n","        model.add(Dense(512, activation='relu'))\n","        model.add(Dense(self.num_actions))\n","\n","        # FOR THEANO: s = tf.placeholder(tf.float32, [None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n","        s = tf.placeholder(tf.float32, [None, FRAME_WIDTH, FRAME_HEIGHT,STATE_LENGTH])\n","        q_values = model(s)\n","\n","        return s, q_values, model\n","\n","    def build_training_op(self, q_network_weights):\n","        a = tf.placeholder(tf.int64, [None])\n","        y = tf.placeholder(tf.float32, [None])\n","\n","        # Convert action to one hot vector\n","        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n","        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n","\n","        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n","        error = tf.abs(y - q_value)\n","        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n","        linear_part = error - quadratic_part\n","        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n","\n","        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n","        grads_update = optimizer.minimize(loss, var_list=q_network_weights)\n","\n","        return a, y, loss, grads_update\n","\n","    def get_initial_state(self, observation, last_observation):\n","        processed_observation = np.maximum(observation, last_observation)\n","        processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n","        state = [processed_observation for _ in range(STATE_LENGTH)]\n","        return np.stack(state, axis=2)\n","        # FOR THEANO: return np.stack(state, axis=0)\n","\n","    def get_action(self, state):\n","        if self.epsilon >= random.random() or self.t < INITIAL_REPLAY_SIZE:\n","            action = random.randrange(self.num_actions)\n","        else:\n","            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n","\n","        # Anneal epsilon linearly over time\n","        if self.epsilon > FINAL_EPSILON and self.t >= INITIAL_REPLAY_SIZE:\n","            self.epsilon -= self.epsilon_step\n","\n","        return action\n","\n","    def run(self, state, action, reward, terminal, observation):\n","        # FOR THEANO: next_state = np.append(state[1:, :, :], observation, axis=0)\n","        next_state = np.append(state[:, :, 1:], processed_observation, axis=2)\n","\n","        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n","        reward = np.clip(reward, -1, 1)\n","\n","        # Store transition in replay memory\n","        self.replay_memory.append((state, action, reward, next_state, terminal))\n","        if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n","            self.replay_memory.popleft()\n","\n","        if self.t >= INITIAL_REPLAY_SIZE:\n","            # Train network\n","            if self.t % TRAIN_INTERVAL == 0:\n","                self.train_network()\n","\n","            # Update target network\n","            if self.t % TARGET_UPDATE_INTERVAL == 0:\n","                self.sess.run(self.update_target_network)\n","\n","            # Save network\n","            if self.t % SAVE_INTERVAL == 0:\n","                save_path = self.saver.save(self.sess, SAVE_NETWORK_PATH + '/' + ENV_NAME, global_step=self.t)\n","                print('Successfully saved: ' + save_path)\n","\n","        self.total_reward += reward\n","        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n","        self.duration += 1\n","\n","        if terminal:\n","            # Write summary\n","            if self.t >= INITIAL_REPLAY_SIZE:\n","                stats = [self.total_reward, self.total_q_max / float(self.duration),\n","                        self.duration, self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL))]\n","                for i in range(len(stats)):\n","                    self.sess.run(self.update_ops[i], feed_dict={\n","                        self.summary_placeholders[i]: float(stats[i])\n","                    })\n","                summary_str = self.sess.run(self.summary_op)\n","                self.summary_writer.add_summary(summary_str, self.episode + 1)\n","\n","            # Debug\n","            if self.t < INITIAL_REPLAY_SIZE:\n","                mode = 'random'\n","            elif INITIAL_REPLAY_SIZE <= self.t < INITIAL_REPLAY_SIZE + EXPLORATION_STEPS:\n","                mode = 'explore'\n","            else:\n","                mode = 'exploit'\n","            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n","                self.episode + 1, self.t, self.duration, self.epsilon,\n","                self.total_reward, self.total_q_max / float(self.duration),\n","                self.total_loss / (float(self.duration) / float(TRAIN_INTERVAL)), mode))\n","\n","            self.total_reward = 0\n","            self.total_q_max = 0\n","            self.total_loss = 0\n","            self.duration = 0\n","            self.episode += 1\n","\n","        self.t += 1\n","\n","        return next_state\n","\n","    def train_network(self):\n","        state_batch = []\n","        action_batch = []\n","        reward_batch = []\n","        next_state_batch = []\n","        terminal_batch = []\n","        y_batch = []\n","\n","        # Sample random minibatch of transition from replay memory\n","        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n","        for data in minibatch:\n","            state_batch.append(data[0])\n","            action_batch.append(data[1])\n","            reward_batch.append(data[2])\n","            next_state_batch.append(data[3])\n","            terminal_batch.append(data[4])\n","\n","        # Convert True to 1, False to 0\n","        terminal_batch = np.array(terminal_batch) + 0\n","\n","        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)})\n","        y_batch = reward_batch + (1 - terminal_batch) * GAMMA * np.max(target_q_values_batch, axis=1)\n","\n","        loss, _ = self.sess.run([self.loss, self.grads_update], feed_dict={\n","            self.s: np.float32(np.array(state_batch) / 255.0),\n","            self.a: action_batch,\n","            self.y: y_batch\n","        })\n","\n","        self.total_loss += loss\n","\n","    def setup_summary(self):\n","        episode_total_reward = tf.Variable(0.)\n","        tf.summary.scalar(ENV_NAME + '/Total_Reward/Episode', episode_total_reward)\n","        episode_avg_max_q = tf.Variable(0.)\n","        tf.summary.scalar(ENV_NAME + '/Average_Max_Q/Episode', episode_avg_max_q)\n","        episode_duration = tf.Variable(0.)\n","        tf.summary.scalar(ENV_NAME + '/Duration/Episode', episode_duration)\n","        episode_avg_loss = tf.Variable(0.)\n","        tf.summary.scalar(ENV_NAME + '/Average_Loss/Episode', episode_avg_loss)\n","        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss]\n","        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n","        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n","        summary_op = tf.summary.merge_all()\n","        return summary_placeholders, update_ops, summary_op\n","\n","    def load_network(self):\n","        checkpoint = tf.train.get_checkpoint_state(SAVE_NETWORK_PATH)\n","        if checkpoint and checkpoint.model_checkpoint_path:\n","            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n","            print('Successfully loaded: ' + checkpoint.model_checkpoint_path)\n","        else:\n","            print('Training new network...')\n","\n","    def get_action_at_test(self, state):\n","        if random.random() <= 0.05:\n","            action = random.randrange(self.num_actions)\n","        else:\n","            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n","\n","        self.t += 1\n","\n","        return action\n","\n","\n","def preprocess(observation, last_observation):\n","    processed_observation = np.maximum(observation, last_observation)\n","    processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n","    return np.reshape(processed_observation, (FRAME_WIDTH, FRAME_HEIGHT,1))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vZz4sHhme9FA","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","## <font color=1334F2>Some Simple Tests to better understand the code</font>\n","---"]},{"metadata":{"id":"cwvvbKfvfRf8","colab_type":"text"},"cell_type":"markdown","source":["- ### Instatiate the environmanet (Atari Game) and the Reinforcement Learning Agent : It will be base on Deep Q-Networks (DQN Agent"]},{"metadata":{"id":"3Un8iBUZXCIS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# REMEMBER: in parameters:\n","# ENV_NAME = 'Breakout-v0'  # Environment name\n","\n","env = gym.make(ENV_NAME)\n","agent = Agent(num_actions=env.action_space.n)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2sOdfU393uaK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# GENERATE SOME INITIAL NO_OP steps for INITIALIZATION:\n","\n","observation = env.reset()\n","for _ in range(random.randint(1, NO_OP_STEPS)):\n","  last_observation = observation\n","  observation, _, _, _ = env.step(0)  # Do nothing"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JChOxSeI3630","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["observation.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_6O0bI6d4ZFK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Plot an OBSERVATION: is a Game Frame:\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.imshow(observation)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gPV16V1jCrXA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# A \"state\" is composed by (in this case) 4 frames with size 84 x 84 (croped,gray, images)\n","\n","# REMEMBER: in parameters:\n","#FRAME_WIDTH = 84  # Resized frame width\n","#FRAME_HEIGHT = 84  # Resized frame height\n","#STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n","\n","state = agent.get_initial_state(observation, last_observation)\n","print('State dimesions',state.shape)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"alKMkLun46a4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["state = agent.get_initial_state(observation, last_observation)\n","\n","print('State dimesions',state.shape)\n","plt.subplot(2,2,1)\n","plt.imshow(state[:,:,0])\n","plt.subplot(2,2,2)\n","plt.imshow(state[:,:,1])\n","plt.subplot(2,2,3)\n","plt.imshow(state[:,:,2])\n","plt.subplot(2,2,4)\n","plt.imshow(state[:,:,3])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xDAFvUEL6e4A","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["last_observation = observation\n","action = agent.get_action(state)\n","print('Action: ',action)\n","observation, reward, terminal, _ = env.step(action)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FZXdlqyl7qBQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# SEE the \"internal\" operation\n","processed_observation = np.maximum(observation, last_observation)\n","plt.subplot(1,3,1)\n","plt.imshow(observation)\n","plt.subplot(1,3,2)\n","plt.imshow(last_observation)\n","plt.subplot(1,3,3)\n","plt.imshow(processed_observation)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oWtxA2HT7aPg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["processed_observation = preprocess(observation, last_observation)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hWL_dMn88PI2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print('Processed observation shape',processed_observation.shape)\n","plt.imshow(processed_observation.reshape((84,84)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XUX-7SpqGHeg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["state.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4nFh00LtGLIQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["processed_observation.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4aKpqqrYp1HC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["state[:, :, 1:].shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"scRdMZ7SGFW-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# SEE HOW a new PROCESSED observation is added as next_state (4 frames)\n","\n","next_state = np.append(state[:, :, 1:], processed_observation, axis=2)\n","\n","state=next_state\n","print('State dimesions',state.shape)\n","plt.subplot(2,2,1)\n","plt.imshow(state[:,:,0])\n","plt.subplot(2,2,2)\n","plt.imshow(state[:,:,1])\n","plt.subplot(2,2,3)\n","plt.imshow(state[:,:,2])\n","plt.subplot(2,2,4)\n","plt.imshow(state[:,:,3])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KnaSMM8fHm9e","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","## <font color=1334F2>NOW DO THE TRAINING...</font>\n","<font color=1334F2 size=4> ..and study [Human-level control through deep reinforcement learning - Presented by Bowen Xu](http://www.teach.cs.toronto.edu/~csc2542h/fall/material/csc2542f16_dqn.pdf)</font>\n","\n","\n","- ### Understand main steps:\n","\n","> ### Process a new observation and feed it into \"agent.run\" that:\n","\n","> 1.- appends it to a new state\n","\n","> 2.- stores it into REPLAY MEMORY\n","\n","> 3.- IF \"replay memory\" larger that NUM_REPLAY_MEMORY trains the \"training network\"\n","\n","> 4.- AFTER a number of steps (TARGET_UPDATE_INTERVAL) the Q network is updated\n","\n","\n","> 5.- SAVE NETWORK every SAVE_INTERVAL interval\n","\n","---"]},{"metadata":{"id":"h3jc5B-Vmayi","colab_type":"text"},"cell_type":"markdown","source":["---\n","## <font color=red>Training will take time!</font>\n","\n","<font color=darkorange size=4>**See Figues from**:<br>\n"," **MAINLY Tatsuya** (tokb23) : https://github.com/tokb23/dqn/blob/master/dqn.py</font>\n","\n","![See Figures](https://raw.githubusercontent.com/tokb23/dqn/master/assets/result.png)\n"]},{"metadata":{"id":"cLCOhf6O3ek4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# TRAINING: fon NUM_EPISODES--------------------\n","\n","for _ in range(NUM_EPISODES):\n","  \n","  terminal = False\n","  observation = env.reset()\n","  for _ in range(random.randint(1, NO_OP_STEPS)):\n","    last_observation = observation\n","    observation, _, _, _ = env.step(0)  # Do nothing\n","  \n","  state = agent.get_initial_state(observation, last_observation)\n","\n","  # While an episode is not finished\n","  while not terminal:\n","    last_observation = observation\n","    action = agent.get_action(state)\n","    observation, reward, terminal, _ = env.step(action)\n","    # env.render()\n","    processed_observation = preprocess(observation, last_observation)\n","  \n","    # Process a new observation and feed it into \"agent.run\" that:\n","    #  1.- appends it to a new state\n","    #  2.- stores it into REPLAY MEMORY\n","    #  3.- IF \"replay memory\" larger that NUM_REPLAY_MEMORY trains the\n","    #      \"training network\"  \n","    #  4.- AFTER a number of steps (TARGET_UPDATE_INTERVAL) the Q network is \n","    #      updated\n","    #  5.- SAVE NETWORK every SAVE_INTERVAL intervals\n","    \n","    state = agent.run(state, action, reward, terminal, processed_observation)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CfAgHZ6ZrI3k","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yd87ETHmKGeM","colab_type":"text"},"cell_type":"markdown","source":["---\n","## <font color=1334F2>FINALLY we can do some \"informal\" tests... </font>\n","\n","\n","- ### <font color=red>but see more formal tests! </font>\n","- ### <font color=red>Hyperparameters tuning! etc.</font>\n"]},{"metadata":{"id":"45lVrrkOpGXq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import time\n","from IPython import display\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","i_render = True\n","\n","NUM_EPISODES_AT_TEST=1\n","\n","for _ in range(NUM_EPISODES_AT_TEST):\n","  tot_reward = 0.0\n","  terminal = False\n","  observation = env.reset()\n","  for _ in range(random.randint(1, NO_OP_STEPS)):\n","    last_observation = observation\n","    observation, _, _, _ = env.step(0)  # Do nothing\n","    \n","  state = agent.get_initial_state(observation, last_observation)\n","  while not terminal:\n","    last_observation = observation\n","    action = agent.get_action_at_test(state)\n","    observation, reward, terminal, _ = env.step(action)\n","    \n","    #  env.render()\n","    if i_render:\n","      print('Predicted action: ',env.unwrapped.get_action_meanings()[action])\n","      #env.render()\n","      plt.imshow(env.render(mode='rgb_array'))\n","      display.display(plt.gcf())\n","      time.sleep(1)\n","      display.clear_output(wait=True)\n","    \n","    processed_observation = preprocess(observation, last_observation)\n","    state = np.append(state[:, :, 1:], processed_observation, axis=2)\n","    \n","    tot_reward += reward\n","  print('Game ended! Total reward: {}'.format(tot_reward))\n","# env.monitor.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NKG1FA__PdAk","colab_type":"text"},"cell_type":"markdown","source":["![Epsilon schedule](https://cdn-images-1.medium.com/max/800/1*KqNaTE9W58I-Y1KLViuoxQ.png)"]},{"metadata":{"id":"V7PXqtaQ1bju","colab_type":"text"},"cell_type":"markdown","source":["## Q-learning intuition.\n","## [Deep reinforcement learning: where to start (playing Catch)](https://medium.freecodecamp.org/deep-reinforcement-learning-where-to-start-291fb0058c01)\n","\n","A good way to understand Q-learning is to compare playing Catch with playing chess. In both games you are given a state s (chess: positions of the figures on the board, Catch: location of the fruit and the basket), on which you have to take an action a (chess: move a figure, Catch: move the basket to the left, right, or stay where you are). As a result there will be some reward r and a new state s’. The problem with both Catch and and chess is that the rewards will not appear immediately after you have taken the action. In Catch, you only earn rewards when the fruits hit the basket or fall on the floor, and in chess you only earn a reward when you win or loose the game. Rewards are _sparsely distributed_, most of the time, r will be 0. When there is a reward, it is not always a result of the action taken immediately before. Some action taken long before might have cause the victory. Figuring out which action is responsible for the reward is often referred to as the _credit assignment problem_.\n","\n","Because rewards are delayed, good chess players do not choose their plays only by the immediate reward, but by the _expected future reward_. They do not only think about whether they can eliminate an opponents figure in the next move, but how taking a certain action now will help them in the long run. \n","In Q-learning, we choose our action based on the highest expected future reward. While in state s, we estimate the future reward for each possible action a. We assume that after we have taken action a and moved to the next state s’, everything works out perfectly. Like in finance, we discount future rewards, since they are uncertain.\n","The expected future reward Q(s,a) given a state s and an action a is therefore the reward r that directly follows from a plus the expected future reward Q(s’,a’) if the optimal action a’ is taken in the following state s’, discounted by the discount factor gamma.\n","\n","Q(s,a) = r + gamma * max Q(s’,a’)\n","\n","Good chess players are very good at estimating future rewards in their head. In other words, their function Q(s,a) is very precise. Most chess practice revolves around developing a better Q function. Players peruse many old games to learn how specific moves played out in the past, and how likely a given action is to lead to victory.\n","\n","But how could we estimate a good function Q? This is where neural networks come into play.\n","\n","## Regression after all\n","\n","When playing, we generate lots of experiences consisting of the initial state s, the action taken a, the reward earned r and the state that followed s’. These experiences are our training data. We can frame the problem of estimating Q(s,a) as a simple regression problem. Given an input vector consisting of s and a the neural net is supposed to predict the a value of Q(s,a) equal to the target: r + gamma * max Q(s’,a’). If we are good at predicting Q(s,a) for different states s and actions a, we have a good approximation of Q. Note that Q(s’,a’) is _also_ a prediction of the neural network we are training. \n","\n","Given a batch of experiences < s, a, r, s’ >, the training process then looks as follows:\n","1. For each possible action a’ (left, right, stay), predict the expected future reward Q(s’,a’) using the neural net\n","2. Choose the highest value of the three predictions max Q(s’,a’)\n","3. Calculate r + gamma * max Q(s’,a’). This is the target value for the neural net\n","4. Train the neural net using the loss function 1/2(predicted_Q(s,a) - target)^2\n","\n","During gameplay, all the experiences are stored in a replay memory. This is the class below. \n","\n","The remember function simply saves an experience to a list.\n","The get_batch function performs steps 1 to 3 of the list above and returns an input and a target vector. The actual training is done in a function discussed below."]},{"metadata":{"id":"XZK3enjDgQS2","colab_type":"text"},"cell_type":"markdown","source":["---\n","##<font color=magenta>Now you can go much further!</font>\n","\n","- ### MuJoCo\n","![MuJoCo](https://blog.openai.com/content/images/2017/05/image7.gif)\n","\n","- ### Robotics\n","![Robotics](http://blog.otoro.net/assets/20171109/jpeg/kuka_img.jpeg)\n","\n","- ### Starcraft II: PySC2 is DeepMind's Python component of the StarCraft II Learning Environment (SC2LE) intended to develop StarCraft II into a rich environment for RL research.\n","![Starcraft2](http://i.dailymail.co.uk/i/pix/2016/11/28/10/3AD1E4E900000578-0-image-a-1_1480329576701.jpg)\n","\n","- ### Intel® Nervana™ - Artificial Intelligence Products Group\n","![texto alternativo](https://raw.githubusercontent.com/NervanaSystems/coach/master/img/carla.gif)\n","\n","\n","- ### Self-driving Cars\n","![Self-driving cars](https://i.ytimg.com/vi/NdbOqNQtAjk/maxresdefault.jpg)\n","\n","- # ... and much more and many different application areas..."]}]}